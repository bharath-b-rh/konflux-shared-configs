name: 'Sync Konflux Pipeline Configurations'
description: 'Intelligently synchronizes local Tekton pipeline configurations with upstream Konflux build-definitions while preserving customizations'
author: 'Red Hat OpenShift Team'

inputs:
  auto_patch_missing_tasks:
    description: 'Automatically patch missing tasks using konflux-pipeline-patcher'
    required: false
    default: 'true'
  update_task_bundle_refs:
    description: 'Update task bundle references to latest versions'
    required: false
    default: 'false'
  github_token:
    description: 'GitHub token for creating PRs'
    required: true
  commit_message_prefix:
    description: 'Prefix for commit messages'
    required: false
    default: 'NO-JIRA: '
  pr_labels:
    description: 'Comma-separated list of labels for PRs'
    required: false
    default: 'dependencies,pipeline-sync,automated-pr'

outputs:
  updated:
    description: 'Whether any pipelines were updated'
    value: ${{ steps.sync.outputs.updated }}
  pipelines_processed:
    description: 'Number of pipelines processed'
    value: ${{ steps.sync.outputs.pipelines_processed }}
  missing_tasks_found:
    description: 'Whether missing tasks were found'
    value: ${{ steps.sync.outputs.missing_tasks_found }}
  missing_tasks_patched:
    description: 'Whether missing tasks were successfully patched'
    value: ${{ steps.sync.outputs.missing_tasks_patched }}
  pr_number:
    description: 'PR number if created'
    value: ${{ steps.create-pr.outputs.pull-request-number }}

runs:
  using: 'composite'
  steps:
    - name: Setup Git
      shell: bash
      run: |
        git config --global user.name 'Pipeline Sync Bot'
        git config --global user.email 'noreply@redhat.com'

    - name: Setup Python dependencies
      shell: bash
      run: |
        python3 -m pip install --user requests PyYAML

    - name: Create pipeline sync script
      shell: bash
      run: |
        cat > /tmp/sync-pipeline-configs.py << 'EOF'
        #!/usr/bin/env python3
        """
        Generic Pipeline Sync Script for GitHub Actions
        """
        import requests
        import yaml
        import json
        import sys
        import os
        import subprocess
        import tempfile
        from typing import Dict, Any, List, Set
        
        # Configuration from inputs
        AUTO_PATCH = "${{ inputs.auto_patch_missing_tasks }}" == "true"
        UPDATE_REFS = "${{ inputs.update_task_bundle_refs }}" == "true"
        
        # Pipeline mappings: upstream_url -> local_path
        PIPELINE_MAPPINGS = {
            'https://raw.githubusercontent.com/konflux-ci/build-definitions/main/pipelines/fbc-builder/fbc-builder.yaml': '.tekton/fbc-build-pipeline.yaml',
            # Add more pipeline mappings here as needed
            # 'https://raw.githubusercontent.com/konflux-ci/build-definitions/main/pipelines/docker-build/docker-build.yaml': '.tekton/docker-build-pipeline.yaml',
        }
        
        def fetch_upstream_pipeline(url: str) -> Dict[str, Any]:
            """Fetch upstream pipeline YAML from GitHub."""
            try:
                response = requests.get(url)
                response.raise_for_status()
                return yaml.safe_load(response.text)
            except requests.RequestException as e:
                print(f"Error fetching upstream pipeline: {e}")
                sys.exit(1)
            except yaml.YAMLError as e:
                print(f"Error parsing upstream YAML: {e}")
                sys.exit(1)
        
        def load_local_pipeline(filepath: str) -> Dict[str, Any]:
            """Load local pipeline YAML file."""
            try:
                with open(filepath, 'r') as f:
                    return yaml.safe_load(f)
            except FileNotFoundError:
                print(f"Local pipeline file not found: {filepath}")
                sys.exit(1)
            except yaml.YAMLError as e:
                print(f"Error parsing local YAML: {e}")
                sys.exit(1)
        
        def save_local_pipeline(filepath: str, data: Dict[str, Any]) -> None:
            """Save updated pipeline to local file."""
            try:
                with open(filepath, 'w') as f:
                    yaml.dump(data, f, default_flow_style=False, sort_keys=False, indent=2)
            except Exception as e:
                print(f"Error saving local pipeline: {e}")
                sys.exit(1)
        
        def remove_taskref_fields(data: Dict[str, Any]) -> Dict[str, Any]:
            """Recursively remove taskRef fields and preserve specific params for comparison."""
            PRESERVE_PARAMS = {'hermetic', 'build-source-image', 'build-args', 'build-platforms'}
            
            if isinstance(data, dict):
                cleaned = {}
                for key, value in data.items():
                    if key == 'taskRef':
                        continue
                    elif key == 'params' and isinstance(value, list):
                        filtered_params = []
                        for param in value:
                            if isinstance(param, dict) and param.get('name') not in PRESERVE_PARAMS:
                                filtered_params.append(remove_taskref_fields(param))
                        cleaned[key] = filtered_params
                    elif isinstance(value, (dict, list)):
                        cleaned[key] = remove_taskref_fields(value)
                    else:
                        cleaned[key] = value
                return cleaned
            elif isinstance(data, list):
                return [remove_taskref_fields(item) for item in data]
            else:
                return data
        
        def get_task_names(pipeline_spec: Dict[str, Any]) -> Set[str]:
            """Extract task names from pipeline spec."""
            task_names = set()
            
            if 'tasks' in pipeline_spec:
                for task in pipeline_spec['tasks']:
                    if 'name' in task:
                        task_names.add(task['name'])
            
            if 'finally' in pipeline_spec:
                for task in pipeline_spec['finally']:
                    if 'name' in task:
                        task_names.add(task['name'])
            
            return task_names
        
        def compare_specs(local_spec: Dict[str, Any], upstream_spec: Dict[str, Any]) -> bool:
            """Compare two pipeline specs, ignoring taskRef fields."""
            local_clean = remove_taskref_fields(local_spec)
            upstream_clean = remove_taskref_fields(upstream_spec)
            
            return json.dumps(local_clean, sort_keys=True) == json.dumps(upstream_clean, sort_keys=True)
        
        def update_pipeline_with_upstream(local_pipeline: Dict[str, Any], 
                                        upstream_pipeline: Dict[str, Any]) -> tuple[Dict[str, Any], bool]:
            """Update local pipeline with upstream values, preserving taskRef fields and specific parameters."""
            updated_pipeline = local_pipeline.copy()
            has_updates = False
            
            PRESERVE_PARAMS = {'hermetic', 'build-source-image', 'build-args', 'build-platforms'}
            
            local_spec = local_pipeline.get('spec', {})
            upstream_spec = upstream_pipeline.get('spec', {})
            
            specs_match = compare_specs(local_spec, upstream_spec)
            
            if not specs_match:
                print("=== DIFFERENCES FOUND ===")
                
                # Update top-level spec fields (except tasks, finally, and params)
                for key, value in upstream_spec.items():
                    if key not in ['tasks', 'finally', 'params']:
                        if key not in local_spec or local_spec[key] != value:
                            print(f"Updating spec.{key}")
                            updated_pipeline['spec'][key] = value
                            has_updates = True
                
                # Handle params - preserve specific parameters while updating others
                if 'params' in upstream_spec:
                    local_params = local_spec.get('params', [])
                    upstream_params = upstream_spec['params']
                    
                    local_params_map = {param.get('name'): param for param in local_params}
                    upstream_params_map = {param.get('name'): param for param in upstream_params}
                    
                    updated_params = []
                    
                    for upstream_param in upstream_params:
                        param_name = upstream_param.get('name')
                        
                        if param_name in PRESERVE_PARAMS and param_name in local_params_map:
                            preserved_param = local_params_map[param_name].copy()
                            updated_params.append(preserved_param)
                            print(f"Preserving local value for param: {param_name}")
                        else:
                            if param_name not in local_params_map or local_params_map[param_name] != upstream_param:
                                print(f"Updating param: {param_name}")
                                has_updates = True
                            updated_params.append(upstream_param)
                    
                    for local_param in local_params:
                        param_name = local_param.get('name')
                        if param_name not in upstream_params_map:
                            print(f"Keeping local-only param: {param_name}")
                            updated_params.append(local_param)
                    
                    updated_pipeline['spec']['params'] = updated_params
                
                # Handle tasks - preserve taskRef but update other fields
                if 'tasks' in upstream_spec:
                    local_tasks = {task['name']: task for task in local_spec.get('tasks', [])}
                    updated_tasks = []
                    
                    for upstream_task in upstream_spec['tasks']:
                        task_name = upstream_task['name']
                        if task_name in local_tasks:
                            updated_task = upstream_task.copy()
                            if 'taskRef' in local_tasks[task_name]:
                                updated_task['taskRef'] = local_tasks[task_name]['taskRef']
                            updated_tasks.append(updated_task)
                        else:
                            updated_tasks.append(upstream_task)
                            print(f"Adding new task: {task_name}")
                            has_updates = True
                    
                    updated_pipeline['spec']['tasks'] = updated_tasks
                
                # Handle finally tasks
                if 'finally' in upstream_spec:
                    local_finally = {task['name']: task for task in local_spec.get('finally', [])}
                    updated_finally = []
                    
                    for upstream_task in upstream_spec['finally']:
                        task_name = upstream_task['name']
                        if task_name in local_finally:
                            updated_task = upstream_task.copy()
                            if 'taskRef' in local_finally[task_name]:
                                updated_task['taskRef'] = local_finally[task_name]['taskRef']
                            updated_finally.append(updated_task)
                        else:
                            updated_finally.append(upstream_task)
                            print(f"Adding new finally task: {task_name}")
                            has_updates = True
                    
                    updated_pipeline['spec']['finally'] = updated_finally
            
            return updated_pipeline, has_updates
        
        def download_pipeline_patcher() -> str:
            """Download the konflux-pipeline-patcher tool."""
            patcher_path = '/tmp/pipeline-patcher'
            
            print("📥 Downloading konflux-pipeline-patcher tool...")
            try:
                response = requests.get('https://github.com/simonbaird/konflux-pipeline-patcher/raw/main/pipeline-patcher')
                response.raise_for_status()
                
                with open(patcher_path, 'wb') as f:
                    f.write(response.content)
                
                os.chmod(patcher_path, 0o755)
                print(f"✅ Pipeline patcher downloaded to {patcher_path}")
                return patcher_path
            except Exception as e:
                print(f"❌ Error downloading pipeline patcher: {e}")
                return None
        
        def patch_missing_tasks(patcher_path: str, pipeline_path: str, missing_tasks: Set[str]) -> bool:
            """Use pipeline-patcher to add missing tasks to the pipeline."""
            if not missing_tasks or not patcher_path:
                return False
            
            print(f"🔧 Patching {len(missing_tasks)} missing tasks: {', '.join(sorted(missing_tasks))}")
            
            task_names = ','.join(sorted(missing_tasks))
            
            try:
                result = subprocess.run([patcher_path, 'patch', pipeline_path, task_names], 
                                      capture_output=True, text=True, check=True)
                
                print(f"✅ Successfully patched missing tasks")
                return True
            except subprocess.CalledProcessError as e:
                print(f"❌ Error patching tasks: {e}")
                return False
        
                 def main():
             print("=== Pipeline Sync Script (GitHub Actions) ===")
             print(f"Processing {len(PIPELINE_MAPPINGS)} pipeline(s)")
             print()
             
             overall_updated = False
             overall_missing_tasks_found = False
             overall_missing_tasks_patched = False
             all_changes = []
             pipelines_processed = 0
             
             for upstream_url, local_path in PIPELINE_MAPPINGS.items():
                 print(f"📄 Processing pipeline: {local_path}")
                 print(f"   Upstream: {upstream_url}")
                 
                 try:
                     # Load pipelines
                     print("   Fetching upstream pipeline...")
                     upstream_pipeline = fetch_upstream_pipeline(upstream_url)
                     
                     print("   Loading local pipeline...")
                     local_pipeline = load_local_pipeline(local_path)
                     
                     # Extract task names
                     local_tasks = get_task_names(local_pipeline.get('spec', {}))
                     upstream_tasks = get_task_names(upstream_pipeline.get('spec', {}))
                     
                     missing_tasks = upstream_tasks - local_tasks
                     
                     print(f"   Local tasks: {len(local_tasks)} tasks")
                     print(f"   Upstream tasks: {len(upstream_tasks)} tasks")
                     
                     if missing_tasks:
                         print(f"   ⚠️  MISSING TASKS: {', '.join(sorted(missing_tasks))}")
                         overall_missing_tasks_found = True
                     
                     # Update pipeline with upstream changes
                     updated_pipeline, has_updates = update_pipeline_with_upstream(local_pipeline, upstream_pipeline)
                     
                     if has_updates:
                         print("   ✅ Updates found - saving updated pipeline...")
                         save_local_pipeline(local_path, updated_pipeline)
                         overall_updated = True
                     else:
                         print("   ✅ No updates needed")
                     
                     # Handle missing tasks
                     missing_tasks_patched = False
                     if missing_tasks and AUTO_PATCH:
                         patcher_path = download_pipeline_patcher()
                         if patcher_path:
                             missing_tasks_patched = patch_missing_tasks(patcher_path, local_path, missing_tasks)
                             if missing_tasks_patched:
                                 overall_missing_tasks_patched = True
                                 overall_updated = True
                     
                     # Track changes for this pipeline
                     pipeline_changes = []
                     if has_updates:
                         pipeline_changes.append(f"Updated {local_path} from upstream")
                     if missing_tasks_patched:
                         pipeline_changes.append(f"Patched {len(missing_tasks)} missing tasks in {local_path}")
                     elif missing_tasks:
                         pipeline_changes.append(f"Found {len(missing_tasks)} missing tasks in {local_path} (auto-patch disabled)")
                     
                     if pipeline_changes:
                         all_changes.extend([f"- {change}" for change in pipeline_changes])
                     
                     pipelines_processed += 1
                     print(f"   ✅ Pipeline {local_path} processed successfully")
                     
                 except Exception as e:
                     print(f"   ❌ Error processing {local_path}: {e}")
                     continue
                 
                 print()
             
             # Set GitHub Action outputs
             with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                 f.write(f"updated={str(overall_updated).lower()}\n")
                 f.write(f"pipelines_processed={pipelines_processed}\n")
                 f.write(f"missing_tasks_found={str(overall_missing_tasks_found).lower()}\n")
                 f.write(f"missing_tasks_patched={str(overall_missing_tasks_patched).lower()}\n")
             
             # Generate changes summary
             changes_summary = "\n".join(all_changes) if all_changes else "No changes made"
             
             with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                 f.write(f"changes_summary<<EOF\n{changes_summary}\nEOF\n")
             
             print(f"=== SYNC COMPLETE ===")
             print(f"Processed {pipelines_processed} pipeline(s)")
             print(f"Updates made: {overall_updated}")
         
         if __name__ == "__main__":
             main()
        EOF
        
        chmod +x /tmp/sync-pipeline-configs.py

    - name: Sync pipeline configurations
      id: sync
      shell: bash
      run: |
        python3 /tmp/sync-pipeline-configs.py

    - name: Create Pull Request
      id: create-pr
      if: steps.sync.outputs.updated == 'true'
      uses: peter-evans/create-pull-request@v5
      with:
        token: ${{ inputs.github_token }}
        commit-message: "${{ inputs.commit_message_prefix }}Sync Konflux pipeline configurations with upstream"
        title: "Sync Konflux pipeline configurations from upstream"
        body: |
          This PR intelligently syncs Tekton pipeline configurations from upstream while preserving local customizations.
          
          **Changes:**
          ${{ steps.sync.outputs.changes_summary }}
          
          **Summary:**
          - 📄 Pipelines processed: ${{ steps.sync.outputs.pipelines_processed }}
          - 🔄 Updates made: ${{ steps.sync.outputs.updated }}
          - ⚠️ Missing tasks found: ${{ steps.sync.outputs.missing_tasks_found }}
          - 🔧 Missing tasks patched: ${{ steps.sync.outputs.missing_tasks_patched }}
          
          **Features:**
          - ✅ Preserves taskRef configurations (bundle URLs, SHA digests)
          - ✅ Preserves custom parameters (hermetic, build-source-image, build-args, build-platforms)
          - ✅ Auto-patches missing tasks: ${{ inputs.auto_patch_missing_tasks }}
          - ✅ Updates task bundle references: ${{ inputs.update_task_bundle_refs }}
          
          This update was automatically generated by the pipeline sync action.
        branch: sync-konflux-pipelines
        delete-branch: true
        labels: ${{ inputs.pr_labels }} 